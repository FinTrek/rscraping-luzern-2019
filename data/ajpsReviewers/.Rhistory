str_extract_all("Phone 150$, PC 690$", "[0-9]+\\$") # example
str_extract("The smallest cities are the best", "\\b[a-z]{1,4}\\b")
str_extract_all("The smallest cities are the best", "\\b[a-z]{1,4}\\b")
str_extract_all("course.txt", ".*?\\.txt$")
str_extract_all(c("course.txt", "log.txt", ".txt", "word.docx"), ".*?\\.txt$")
str_extract_all(c("course.txt", "log.txt", ".txt", "word.docx", "file1.txt, file2.txt"), ".*?\\.txt$")
str_extract_all(c("course.txt", "log.txt", ".txt", "word.docx", "file1.txt and some other file2.txt), ".*?\\.txt$")
str_extract_all(c("course.txt", "log.txt", ".txt", "word.docx", "file1.txt and some other file2.txt"), ".*?\\.txt$")
str_extract_all(c("course.txt", "log.txt", ".txt", "word.docx", "file1.txt and some other file2.txt"), ".*\\.txt$")
str_extract_all(c("course.txt", "log.txt", ".txt", "word.docx", "file1.txt and some other file2.txt"), "file.*?\\.txt$")
str_extract_all(c("course.txt", "log.txt", ".txt", "word.docx", "file1.txt and some other file2.txt"), "file.*?\\.txt")
str_extract_all(c("course.txt", "log.txt", "hello_txt.txt"), ".*?\\.txt$")
str_extract_all(c("course.txt", "log.txt", "log..txt"), ".*?\\.txt$")
str_extract_all("12/01/2019 to 12/01/19", "\\d{2}/\\d{2}/\\d{2,4}")
str_extract_all("<xy>hello</xy>","<(.+?)>.+?</\\1>")
str_extract_all("<li>hello</li>","<(.+?)>.+?</\\1>")
str_extract_all("log.txt, example.html, bla.txt2", ".*?\\.txt$")
str_extract_all("log.txt, example.html, bla.txt2", ".*?\\.txt$")
str_extract_all(c("log.txt", "example.html", "bla.txt2"), ".*?\\.txt$")
str_extract_all("log.txt, example.html, bla.txt2", ".*?\\.txt$")
?str_replace
source("packages.r")
# parse with read_html
parsed_doc <- read_html("https://www.facebook.com")
parsed_doc
# inspect parsed object
class(parsed_doc)
html_structure(parsed_doc)
as_list(parsed_doc)
# import running example
parsed_doc <- read_html("http://www.r-datacollection.com/materials/ch-4-xpath/fortunes/fortunes.html")
parsed_doc
# absolute paths
html_nodes(parsed_doc, xpath = "/html/body/div/p/i")
# relative paths
html_nodes(parsed_doc, xpath = "//body//p/i")
html_nodes(parsed_doc, xpath = "//p/i")
html_nodes(parsed_doc, xpath = "//i")
# wildcard (for ONE node)
html_nodes(parsed_doc, xpath = "/html/body/div/*/i")
html_nodes(parsed_doc, xpath = "/html/body/*/i") # does not work
# ancestor
html_nodes(parsed_doc, xpath = "//a/ancestor::div")
html_nodes(parsed_doc, xpath = "//a/ancestor::div//i")
# ancestor
html_nodes(parsed_doc, xpath = "//a/ancestor::div")
# sibling
html_nodes(parsed_doc, xpath = "//p/preceding-sibling::h1")
# Parent
html_nodes(parsed_doc, xpath = "//title/parent::*")
# numeric
html_nodes(parsed_doc, xpath = "//div/p[1]")
html_nodes(parsed_doc, xpath =  "//div/p[last()]")
html_nodes(parsed_doc, xpath = "//div[count(.//a)>0]")
html_nodes(parsed_doc, xpath = "//div[count(./@*)>2]")
html_nodes(parsed_doc, xpath = "//*[string-length(text())>50]")
# text-based
html_nodes(parsed_doc, xpath = "//div[@date='October/2011']")
html_nodes(parsed_doc, xpath = "//*[contains(text(), 'magic')]")
html_nodes(parsed_doc, xpath = "//div[substring-after(./@date, '/')='2003']//i")
html_nodes(parsed_doc, xpath = "//title")
# values
html_nodes(parsed_doc, xpath = "//title") %>% html_text()
# values
html_nodes(parsed_doc, xpath = "//title") %>% html_text()
# attributes
html_nodes(parsed_doc, xpath = "//div") %>% html_attrs() # all attributes
html_nodes(parsed_doc, xpath = "//div") %>% html_attr("lang") # single attribute
# inspect form
session <- html_session("http://www.google.com")
source("packages.r")
# inspect form
session <- html_session("http://www.google.com")
search <- html_form(session)[[1]]
search
search <- html_form(session)
search
search <- html_form(session)[[1]]
search
# set form parameters
form <- set_values(search, q = "kneipen kreuzberg")
form
# submit form
google_search <- submit_form(session, form)
google_search
hits_text <- html_nodes(url_parsed, xpath = "//*[@class='r']//a") %>% html_text()
# retrieve results
url_parsed <- read_html(google_search)
hits_text <- html_nodes(url_parsed, xpath = "//*[@class='r']//a") %>% html_text()
hits_text
hits_links <- html_nodes(url_parsed, xpath = "//*[@class='r']//a") %>% html_attr("href")
hits_links
# set form parameters
form <- set_values(search, q = "kneipen kreuzberg", start = "30")
## another example: WordNet Search
# inspect webpage
url <- "http://wordnetweb.princeton.edu/perl/webwn"
browseURL(url)
## another example: WordNet Search
# inspect webpage
url <- "http://wordnetweb.princeton.edu/perl/webwn"
url_parsed <- read_html(url)
html_form(url_parsed)
wordnet <- html_form(url_parsed)[[1]]
wordnet_form <- set_values(wordnet, s = "data")
wordnet_search <- submit_form(session, wordnet_form)
uastring <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
session <- html_session(url, user_agent(uastring))
wordnet_form <- set_values(wordnet, s = "data")
wordnet_search <- submit_form(session, wordnet_form)
url_parsed <- read_html(wordnet_search)
url_parsed %>% html_nodes(xpath = "//li") %>% html_text()
wordnet_form <- set_values(wordnet, s = "data", o2 = "1")
wordnet_search <- submit_form(session, wordnet_form)
url_parsed <- read_html(wordnet_search)
url_parsed %>% html_nodes(xpath = "//li") %>% html_text()
## POST forms
# goal: gathering data from read-able at http://read-able.com/
url <- "http://read-able.com/"
browseURL(url)
url_parsed <- read_html(url)
html_form(url_parsed)
readable <- html_form(url_parsed)[[2]]
sentence <- '"It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit facts." - Arthur Conan Doyle, Sherlock Holmes'
readable_form <- set_values(readable, directInput = sentence)
readable_form
session <- html_session(url, user_agent(uastring))
readable_search <- submit_form(session, readable_form)
url_parsed <- read_html(readable_search)
html_table(url_parsed)
## POST forms
# goal: gathering data from read-able at http://read-able.com/
url <- "http://read-able.com/"
url_parsed <- read_html(url)
html_form(url_parsed)
readable <- html_form(url_parsed)[[2]]
sentence <- '"It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit facts." - Arthur Conan Doyle, Sherlock Holmes'
readable_form <- set_values(readable, directInput = sentence)
session <- html_session(url, user_agent(uastring))
readable_search <- submit_form(session, readable_form)
url_parsed <- read_html(readable_search)
html_table(url_parsed)
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
f1 <- function(x) {
log(x)
10
}
f1("x")
# ignore error
f1 <- function(x) {
try(log(x))
10
}
f1("x")
# suppress error message
f1 <- function(x) {
try(log(x), silent = TRUE)
10
}
f1("x")
# pass block of code to try()
try({
a <- 1
b <- "x"
a + b
})
# capture the output of try()
success <- try(1 + 2)
failure <- try("a" + "b")
class(success)
class(failure)
# test for "try-error" class
is.error <- function(x) inherits(x, "try-error")
succeeded <- !sapply(results, is.error)
# use try() when applying a function to multiple elements in a list
elements <- list(1:10, c(-1, 10), c(T, F), letters)
results <- lapply(elements, log)
results <- lapply(elements, function(x) try(log(x)))
results
# test for "try-error" class
is.error <- function(x) inherits(x, "try-error")
succeeded <- !sapply(results, is.error)
str(results[succeeded])
str(elements[!succeeded])
# use try() together with a default value if expression fails
default <- NULL
try(default <- read.csv("possibly-bad-input.csv"), silent = TRUE)
# even easier with failwith()
safe_fun <- plyr::failwith(default = NULL, f = log, quiet = FALSE)
show_condition <- function(code) {
tryCatch(code,
error = function(c) "error",
warning = function(c) "warning",
message = function(c) "message" )
}
show_condition(stop("!"))
show_condition(warning("?!"))
show_condition(message("?"))
# if no condition is captured, tryCatch returns the value of the input
show_condition(10+5)
# function that returns the mean of a vector
my_mean <- function(my_vector) {
mean <- sum(my_vector)/length(my_vector)
mean
}
library(tidyverse)
library(rvest)
library(pdftools)
# devtools::install_github("hrbrmstr/nominatim")
library(nominatim)
library(OpenStreetMap)
install.packages("rJava")
library(OpenStreetMap)
source("packages.r")
## api key (example below is not a real key)
load("/Users/simonmunzert/twitterkeys.RDa") # <--- adapt path here; see above!
## register app
twitter_token <- create_token(
app = appname,
consumer_key = TwitterToR_twitterkey,
consumer_secret = TwitterToR_twittersecret,
access_token = TwitterToR_accesstoken,
access_secret = TwitterToR_accesssecret,
set_renv = FALSE)
## name assigned to created app
appname <- "TwitterToR" # <--- add your Twitter App name here!
## register app
twitter_token <- create_token(
app = appname,
consumer_key = TwitterToR_twitterkey,
consumer_secret = TwitterToR_twittersecret,
access_token = TwitterToR_accesstoken,
access_secret = TwitterToR_accesssecret,
set_renv = FALSE)
## check if everything worked
rt <- search_tweets("merkel", n = 200, token = twitter_token)
View(rt)
## check if everything worked
rt <- search_tweets("strache", n = 200, token = twitter_token)
View(rt)
source("packages.r")
# example
raw.data <- "555-1239Moe Szyslak(636) 555-0113Burns, C. Montgomery555-6542Rev. Timothy Lovejoy555 8904Ned Flanders636-555-3226Simpson, Homer5553642Dr. Julius Hibbert"
name <- unlist(str_extract_all(raw.data, "[[:alpha:]., ]{2,}"))
name
phone <- unlist(str_extract_all(raw.data, "\\(?(\\d{3})?\\)?(-| )?\\d{3}(-| )?\\d{4}"))
phone
data.frame(name = name, phone = phone)
# running example
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
# self match
str_extract(example.obj, "small")
str_extract(example.obj, "banana")
str_extract(example.obj, "e")
str_extract_all(example.obj, "e")
str_extract_all(example.obj, "e") %>% unlist()
# multiple matches
(out <- str_extract_all(c("text", "manipulation", "basics"), "a"))
# case sensitivity
str_extract(example.obj, "small")
str_extract(example.obj, "SMALL")
str_extract(example.obj, "SMALL", ignore_case = TRUE)
str_extract(example.obj, regex("SMALL", ignore_case = TRUE))
# match empty space
str_extract(example.obj, "mall sent")
# match the beginning of a string
str_extract(example.obj, "^1")
str_extract(example.obj, "^2")
# match the end of a string
str_extract(example.obj, "sentence$")
str_extract(example.obj, "sentence.$")
# pipe operator
unlist(str_extract_all(example.obj, "tiny|sentence"))
# wildcard
str_extract(example.obj, "sm.ll")
# wildcard
str_extract(example.obj, "sm\\.ll")
str_extract(example.obj, "sentence\\.$")
# character class
str_extract(example.obj, "sm[abc]ll")
# character class: range
str_extract(example.obj, "sm[a-p]ll")
# character class: additional characters
unlist(str_extract_all(example.obj, "[uvw. ]"))
# pre-defined character classes
unlist(str_extract_all(example.obj, "[:punct:]"))
# pre-defined character classes
unlist(str_extract_all(example.obj, "[:alpha:]"))
unlist(str_extract_all(example.obj, "[[:punct:]ABC]"))
unlist(str_extract_all(example.obj, "[^[:alnum:]]"))
# for more character classes, see
?base::regex
# additional shortcuts
unlist(str_extract_all(example.obj, "\\w+"))
# word edges
unlist(str_extract_all(example.obj, "e\\b"))
# word edges
unlist(str_extract_all(example.obj, "e\\b"))
unlist(str_extract_all(example.obj, "e\\B"))
# quantifier
str_extract(example.obj, "s[:alpha:][:alpha:][:alpha:]l")
str_extract(example.obj, "s[:alpha:]{3}l")
str_extract(example.obj, "s[:alpha:]{1,3}l")
str_extract(example.obj, "s[:alpha:]{1,}l")
str_extract(example.obj, "s...l")
str_extract(example.obj, "A.+sentence")
str_extract_all(example.obj, "A.+?sentence")
# quantifier with pattern sequence
unlist(str_extract_all(example.obj, "(.en){1,5}"))
unlist(str_extract_all(example.obj, ".en{1,5}"))
# quantifier with pattern sequence
unlist(str_extract_all(example.obj, "(.en){1,4}"))
# quantifier with pattern sequence
unlist(str_extract_all(example.obj, "(.en){1,3}"))
# quantifier with pattern sequence
unlist(str_extract_all(example.obj, "(.en){1,2}"))
# quantifier with pattern sequence
unlist(str_extract_all(example.obj, "(.en){1,1}"))
# quantifier with pattern sequence
unlist(str_extract_all(example.obj, "(.en){1,5}"))
# quantifier with pattern sequence
unlist(str_extract_all(example.obj, "(.en){1,6}"))
# quantifier with pattern sequence
unlist(str_extract_all(example.obj, "(.en){1,5}"))
# quantifier with pattern sequence
unlist(str_extract_all(example.obj, "(.enc){1,5}"))
# quantifier with pattern sequence
unlist(str_extract_all(example.obj, "(.ence){1,5}"))
# meta characters
unlist(str_extract_all(example.obj, "\\."))
example.obj <- "1. A small semtence. - 2. Another tiny sentence."
# quantifier with pattern sequence
unlist(str_extract_all(example.obj, "(.en){1,5}"))
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
# meta characters
unlist(str_extract_all(example.obj, "\\."))
unlist(str_extract_all(example.obj, fixed(".")))
unlist(str_extract_all(example.obj, "[13-]"))
unlist(str_extract_all(example.obj, "[1-3-]"))
# backreferencing
str_extract(example.obj, "([:alpha:]).+?\\1")
str_extract(example.obj, "(\\b[a-z]+\\b).+?\\1")
# assertions
unlist(str_extract_all(example.obj, "(?<=2. ).+")) # positive lookbehind: (?<=...)
unlist(str_extract_all(example.obj, ".+(?=2)")) # positive lookahead (?=...)
unlist(str_extract_all(example.obj, "(?<!Blah )tiny.+")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, "sentence.+(?!Bla)")) # negative lookahead (?!...)
# do you think you can master regular expressions now?
browseURL("http://stackoverflow.com/questions/201323/using-a-regular-expression-to-validate-an-email-address/201378#201378") # think again
phone <- unlist(str_extract_all(raw.data, "\\(?(\\d{3})?\\)?(-| )?\\d{3}(-| )?\\d{4}"))
phone
example.obj
# locate
str_locate(example.obj, "tiny")
# substring extraction
str_sub(example.obj, start = 35, end = 38)
# replacement
str_sub(example.obj, 35, 38) <- "huge"
example.obj
str_replace(example.obj, pattern = "huge", replacement = "giant")
?str_replace
str_split(example.obj, "-")
# splitting
str_split(example.obj, "-") %>% unlist
?magrittr
unlist(str_split(example.obj, "-"))
# splitting
str_split(example.obj, "-") %>% unlist
# manipulate multiple elements; example
(char.vec <- c("this", "and this", "and that"))
# detection
str_detect(char.vec, "this")
# keep strings matching a pattern
str_subset(char.vec, "this") # wrapper around x[str_detect(x, pattern)]
# counting
str_count(char.vec, "a")
str_count(char.vec, "\\w+")
str_length(char.vec)
# stringr is built on top of the stringi package.
# stringr is convenient because it exposes a minimal set of functions, which have been carefully picked to handle the most common string manipulation functions.
# stringi is designed to be comprehensive. It contains almost every function you might ever need: stringi has 234 functions (compare that to stringr's 42)
# packages work very similarly; translating knowledge is easy (try stri_ instead of str_)
library(stringi)
?stri_count_words
example.obj
stri_count_words(example.obj)
stri_stats_latex(example.obj)
stri_stats_general(example.obj)
stri_escape_unicode("\u00b5")
stri_unescape_unicode("\u00b5")
stri_rand_lipsum(3)
stri_rand_shuffle("hello")
stri_rand_strings(100, 10, pattern = "[washington]")
## tasks ------------------------
# downloading PDF files
# importing them into R (as plain text)
# extract information via regex
# geocoding
## packages ---------------------
library(tidyverse)
library(rvest)
library(pdftools)
# devtools::install_github("hrbrmstr/nominatim")
library(nominatim)
wd <- ("data/ajpsReviewers")
dir.create(wd)
setwd(wd)
wd <- ("../data/ajpsReviewers")
dir.create(wd)
setwd(wd)
## step 1: inspect page
url <- "http://ajps.org/list-of-reviewers/"
browseURL(url)
## step 2: retrieve pdfs
# get page
content <- read_html(url)
# get anchor (<a href=...>) nodes via xpath
anchors <- html_nodes(content, xpath = "//a")
# get value of anchors' href attribute
hrefs <- html_attr(anchors, "href")
# filter links to pdfs
pdfs <- hrefs[ str_detect(basename(hrefs), ".*\\d{4}.*pdf") ]
pdfs
# define names for pdfs on disk
pdf_names <- str_extract(basename(pdfs), "\\d{4}") %>% paste0("reviewers", ., ".pdf")
pdf_names
# download pdfs
for(i in seq_along(pdfs)) {
download.file(pdfs[i], pdf_names[i], mode="wb")
}
library(pdftools)
## step 3: import pdf
rev_raw <- pdftools::pdf_text("reviewers2015.pdf")
class(rev_raw)
rev_raw[1]
rev_raw %>% str_split("\\n")
## step 4: tidy data
rev_all <- rev_raw %>% str_split("\\n") %>% unlist
rev_all
head(rev_all)
surname <- str_extract(rev_all, "[[:alpha:]-]+")
surname
## step 3: import pdf
rev_raw <- pdftools::pdf_text("reviewers2015.pdf")
class(rev_raw)
class(rev_raw)
rev_raw[1]
## step 4: tidy data
rev_all <- rev_raw %>% str_split("\\n") %>% unlist
surname <- str_extract(rev_all, "[[:alpha:]-]+")
rev_all
prename <- str_extract(rev_all, " [.[:alpha:]]+")
prename
rev_df <- data.frame(raw = rev_all, surname = surname, prename = prename, stringsAsFactors = F)
View(rev_df)
rev_df$institution <- NA
prename
prename <- str_extract(rev_all, " [[:alpha:].]+")
prename
head(rev_df$raw)
for(i in 1:nrow(rev_df)) {
rev_df$institution[i] <- rev_df$raw[i] %>% str_replace(rev_df$surname[i], "") %>% str_replace(rev_df$prename[i], "") %>% str_trim()
}
View(rev_df)
rev_df <- rev_df[-c(1,2),]
rev_df <- rev_df[!is.na(rev_df$surname),]
head(rev_df)
unique_institutions <- unique(rev_df$institution)
unique_institutions <- unique_institutions[!is.na(unique_institutions)]
pos <- data.frame(lon = NA, lat = NA)
if (!file.exists("institutions2015_geo.RData")){
for (i in 1:length(unique_institutions)) {
pos[i,] <- try(nominatim::osm_search(unique_institutions[i], key = openstreetmap) %>% dplyr::select(lon, lat))
}
pos$institution <- unique_institutions
pos <- filter(pos, !str_detect(lon, "Error"))
pos$lon <- as.numeric(pos$lon)
pos$lat <- as.numeric(pos$lat)
save(pos, file="institutions2015_geo.RData")
} else {
load("institutions2015_geo.RData")
}
head(pos)
rev_geo <- merge(rev_df, pos, by = "institution", all = T)
## step 6: plot reviewers, worldwide
mapWorld <- borders("world")
map <-
ggplot() +
mapWorld +
geom_point(aes(x=rev_geo$lon, y=rev_geo$lat) ,
color="#F54B1A90", size=1,
na.rm=T) +
theme_bw() +
coord_map(xlim=c(-180, 180), ylim=c(-70,80))
map
str_extract_all("Phone 150$, PC 690$", "[0-9]+\\$") # example
